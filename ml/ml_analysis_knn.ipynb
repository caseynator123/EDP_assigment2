{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Book recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Objective of this notebook will be to explore the data sets and then create a model to recommend book<br> stores what books to stock up on depending on the bookstore's location and the demographic(age) of that location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.normpath(os.getcwd() + os.sep + os.pardir) + \"/data/cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.path.join(os.path.normpath(os.getcwd() + os.sep + os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(path + '/BX-Ratings.csv')\n",
    "print(ratings.dtypes)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(path + '/BX-Users.csv')\n",
    "print(users.dtypes)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(users['User-Age'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of User Ages')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0,80)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(path + '/BX-Books.csv')\n",
    "print(books.dtypes)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should only use ratings that are of books in our data set of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_new = ratings[ratings.ISBN.isin(books.ISBN)]\n",
    "ratings.shape,ratings_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ratings and ratings_new have the same shape so we can assume that all the ratings are of books in our books data set.<br>\n",
    "Thats great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our ratings need to be from users in our user data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of dataset before dropping\",ratings_new.shape)\n",
    "ratings_new = ratings_new[ratings_new['User-ID'].isin(users['User-ID'])]\n",
    "print(\"shape of dataset after dropping\",ratings_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so all the ratings we got have their corrosponding user data, nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_counts = ratings['Book-Rating'].value_counts(sort=False)\n",
    "\n",
    "ratings_counts_sorted = ratings_counts.sort_index()\n",
    "\n",
    "plt.rc(\"font\", size=15)\n",
    "ratings_counts_sorted.plot(kind='bar')\n",
    "plt.title('Rating Distribution\\n')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the ratings are very heavily negatively skewed, this means that people will generally review books they like rather than books the don't like more often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top 5 books that have been rated the most by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_count = pd.DataFrame(ratings.groupby('ISBN')['Book-Rating'].count())\n",
    "rating_count.sort_values('Book-Rating', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rated_books = pd.DataFrame(['0316666343', '0971880107', '0385504209', '0312195516', '0060928336'], index=np.arange(5), columns = ['ISBN'])\n",
    "most_rated_books_summary = pd.merge(most_rated_books, books, on='ISBN')\n",
    "most_rated_books_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for intuition let's see the distribution of ratings for the top book: \"The Lovely Bones: A Novel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isbn_0316666343_df = ratings[ratings['ISBN'] == '0316666343']\n",
    "\n",
    "isbn_0316666343_df_counts = isbn_0316666343_df['Book-Rating'].value_counts(sort=False)\n",
    "\n",
    "isbn_0316666343_df_sorted = isbn_0316666343_df_counts.sort_index()\n",
    "\n",
    "plt.rc(\"font\", size=15)\n",
    "isbn_0316666343_df_sorted.plot(kind='bar')\n",
    "plt.title('Rating Distribution\\n')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the same trends still hold thats great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new collumn in our ratings df for average rating of that book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column Rating average \n",
    "ratings['Avg_Rating']=ratings.groupby('ISBN')['Book-Rating'].transform('mean')\n",
    "# Create column Rating sum\n",
    "ratings['Times been Rated']=ratings.groupby('ISBN')['Book-Rating'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now let's merge the data sets together for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Dataset=users.copy()\n",
    "Final_Dataset=pd.merge(Final_Dataset,ratings,on='User-ID')\n",
    "Final_Dataset=pd.merge(Final_Dataset,books,on='ISBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = Final_Dataset.isna().sum()\n",
    "total_values = len(Final_Dataset)\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_values_percentage = (missing_values_count / total_values) * 100\n",
    "\n",
    "# Creating a new DataFrame to store the missing values count and percentage\n",
    "missing_values_df = pd.DataFrame({'Column': missing_values_count.index, \n",
    "                                  'Missing Values': missing_values_count.values,\n",
    "                                  'Missing Values (%)': missing_values_percentage.values})\n",
    "\n",
    "missing_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are a lot of missing values in the location collumns. Since we are trying to train our model to give recommendations based on location, I think that we should omit these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Dataset.dropna(inplace=True)\n",
    "missing_values_count = Final_Dataset.isna().sum()\n",
    "total_values = len(Final_Dataset)\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_values_percentage = (missing_values_count / total_values) * 100\n",
    "\n",
    "# Creating a new DataFrame to store the missing values count and percentage\n",
    "missing_values_df = pd.DataFrame({'Column': missing_values_count.index, \n",
    "                                  'Missing Values': missing_values_count.values,\n",
    "                                  'Missing Values (%)': missing_values_percentage.values})\n",
    "\n",
    "missing_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorising by age and location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to help bookstore owners pick new books to purchase for their store, we need to find out what books the people in that area like. We will take an approach that makes artificial users which will be a collation separated by area and age. Then we will feed this into a model that can tell us what type of books to recommend to this demographic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location Categorisation\n",
    "\n",
    "Let's see how the locations are distributed to get a feel on how to categorise location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = Final_Dataset['User-Country'].value_counts()\n",
    "\n",
    "# Plotting the results\n",
    "country_counts.plot(kind='bar')\n",
    "plt.title('Number of People in Each Country')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this doesn't tell us much, it's clear there are a few countries with far greater number of users. It wouldn't be apprpriate to group all these users together.\n",
    "So, we shall then reduce the scale for these countreis into states or cities. Lets first find out which cities are the greatest and least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(country_counts))\n",
    "country_counts.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly there are alot of countries with only one user and this won't be enough to get an idea of what kind of books that country/demographic enjoy. We will omit these countries in our data set training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts_df = Final_Dataset.groupby('User-Country').size()\n",
    "\n",
    "# Filter countries with 5 or less users\n",
    "countries_to_drop = country_counts_df[country_counts <= 5].index\n",
    "rows_to_drop = Final_Dataset[Final_Dataset['User-Country'].isin(countries_to_drop)].index\n",
    "Final_Dataset = Final_Dataset.drop(rows_to_drop)\n",
    "\n",
    "#Recount\n",
    "country_counts = Final_Dataset['User-Country'].value_counts()\n",
    "print(len(country_counts[(country_counts > 5) & (country_counts < 30)]))\n",
    "country_counts.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will categorize via this method:<br>\n",
    "If the country has less than 30 users we group by country.<br>\n",
    "for countries with between 30 to 200 users we will group by state.<br>\n",
    "for countries with 200 to 500 users we will group by city.<br>\n",
    "for countries with >500 we will group by city then by categorical age(young, middle, old.)<br>\n",
    "\n",
    "The objective is to get roughly the same number of users and reviews in each section such that the range of books is not too wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_lt_30_list = country_counts[country_counts < 30].index\n",
    "countries_30_200_list =  country_counts[(country_counts>=30) & (country_counts <200)].index\n",
    "countries_200_500_list = country_counts[(country_counts>=200) & (country_counts<500)].index\n",
    "countries_gt_500_list = country_counts[(country_counts>500)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30 = Final_Dataset[Final_Dataset['User-Country'].isin(countries_lt_30_list)]\n",
    "df_30_200 = Final_Dataset[Final_Dataset['User-Country'].isin(countries_30_200_list)]\n",
    "df_200_500 = Final_Dataset[Final_Dataset['User-Country'].isin(countries_200_500_list)]\n",
    "df_500 = Final_Dataset[Final_Dataset['User-Country'].isin(countries_gt_500_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df_30['User-Country']\n",
    "new_ids_30 = countries + '-Na-Na-Na'\n",
    "df_30['User-ID'] = new_ids_30\n",
    "\n",
    "df_30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df_30_200['User-Country']\n",
    "states = df_30_200['User-State']\n",
    "new_ids_30_200 = countries + '-'+ states +'-Na-Na'\n",
    "df_30_200['User-ID'] = new_ids_30_200\n",
    "\n",
    "df_30_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df_200_500['User-Country']\n",
    "states = df_200_500['User-State']\n",
    "cities = df_200_500['User-City']\n",
    "new_ids_200_500 = countries + '-'+ states +'-' + cities + '-Na'\n",
    "df_200_500['User-ID'] = new_ids_200_500\n",
    "\n",
    "df_200_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for age grouping we need to find a way to separate the different ages, lets do some quick analysis for the ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_33 = np.percentile(Final_Dataset['User-Age'], 18)\n",
    "\n",
    "percentile_66 = np.percentile(Final_Dataset['User-Age'], 82)\n",
    "\n",
    "print(\"33rd percentile of age:\", percentile_33)\n",
    "print(\"66th percentile of age:\", percentile_66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(Final_Dataset['User-Age'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of User Ages')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution for age is approximately normally distributed, so let's split the distribution into three parts:<br>\n",
    "\n",
    "1. young people 0 - 28\n",
    "2. middle aged 29 - 43\n",
    "3. Old 44 - &infin;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bins = [0, 28, 43, np.inf]\n",
    "age_labels = ['Young', 'Middle-aged', 'Old']\n",
    "df_500['Age-Category'] = pd.cut(df_500['User-Age'], bins=age_bins, labels=age_labels, right=False)\n",
    "df_500['Age-Category'] = df_500['Age-Category'].astype('object')\n",
    "\n",
    "df_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df_500['User-Country']\n",
    "states = df_500['User-State']\n",
    "cities = df_500['User-City']\n",
    "age = df_500['Age-Category']\n",
    "new_ids_500 = countries + '-'+ states +'-' + cities + '-' + age\n",
    "df_500['User-ID'] = new_ids_500\n",
    "\n",
    "df_500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_final_data = pd.concat([df_30, df_30_200, df_200_500, df_500], ignore_index=True)\n",
    "demographic_final_data.rename(columns={'User-ID': 'demographic'}, inplace=True)\n",
    "\n",
    "demographic_final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_counts = demographic_final_data.groupby('demographic').size()\n",
    "print(len(user_id_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, so now we have the same dataframe, but this has in terms of the demographic which will be more useful for bookstores and booksellers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book recommendation system\n",
    "\n",
    "This section we will try and transform the data to make KNN more plausible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a matrix of ratings and books to see what books get ratings from similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix = ratings.pivot(index='User-ID', columns='ISBN', values='Book-Rating')\n",
    "userID = ratings_matrix.index\n",
    "ISBN = ratings_matrix.columns\n",
    "print(ratings_matrix.shape)\n",
    "ratings_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = ratings_matrix.shape[0]\n",
    "n_books = ratings_matrix.shape[1]\n",
    "print (n_users, n_books)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix up those nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix.fillna(0, inplace = True)\n",
    "ratings_matrix = ratings_matrix.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = 1.0-len(ratings)/float(ratings.shape[0]*n_books)\n",
    "print('Matrix Sparsity:' +  str(sparsity*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very high sparsity and something we need to fix to get our knn model to work better,<br> to fix this we should omit books with few ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ratings = pd.read_csv(path + '/BX-Ratings.csv')\n",
    "\n",
    "\n",
    "combine_book_rating = pd.merge(raw_ratings, books, on = 'ISBN')\n",
    "combine_book_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only really care about the Book ratings and Book titles here so let's remove some distracting collumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Book-Author', 'Book-Info', 'Year-Of-Publication', 'Book-Publisher', 'Book-Vector', 'Year-Of-Publication-Group', 'Year-Of-Publication-Group-Encoded']\n",
    "combine_book_rating = combine_book_rating.drop(cols_to_drop, axis = 1)\n",
    "combine_book_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's group it by titles and find out how many times each title was reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ratings = combine_book_rating.groupby(by=['Book-Title'])['Book-Rating'].count()\n",
    "reset_index = grouped_ratings.reset_index()\n",
    "renamed_columns = reset_index.rename(columns={'Book-Rating': 'Total-Rating-Count'})\n",
    "book_ratingcount = renamed_columns[['Book-Title', 'Total-Rating-Count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_count = book_ratingcount.sort_values(by='Total-Rating-Count', ascending=False)\n",
    "sorted_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts = combine_book_rating.merge(book_ratingcount, left_on = 'Book-Title', right_on = 'Book-Title', how = 'inner' )\n",
    "\n",
    "merged_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now let's analyse our distribution and take books that have a significant ammount of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_ratingcount['Total-Rating-Count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_ratingcount['Total-Rating-Count'].quantile(np.arange(.9,1,.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that only a few books have significant amount of ratings. Because we have so many books, let's take the top 4% of books, that is the books that receive 47 or more ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_threshold = 47\n",
    "rating_popular_book = merged_counts[merged_counts['Total-Rating-Count'] >= popularity_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_removed = len(merged_counts) - len(rating_popular_book)\n",
    "print('ratings_removed =', ratings_removed)\n",
    "\n",
    "rating_popular_book.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check and remove any duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = rating_popular_book.duplicated(['User-ID', 'Book-Title'])\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "print(\"Number of duplicate rows:\", num_duplicates)\n",
    "print(rating_popular_book.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not rating_popular_book[rating_popular_book.duplicated(['User-ID', 'Book-Title'])].empty:\n",
    "    rating_popular_book = rating_popular_book.drop_duplicates(['User-ID', 'Book-Title'])\n",
    "print(rating_popular_book.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And Now we have our Matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_matrix = rating_popular_book.pivot(index = 'Book-Title',columns = 'User-ID', values = 'Book-Rating').fillna(0)\n",
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = user_rating_matrix.size\n",
    "num_zero_elements = np.count_nonzero(user_rating_matrix == 0)\n",
    "sparsity = num_zero_elements / total_elements\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Sparsity is barely acceptable, we could increase our threshhold but this will come at the cost of ignoring less popular books and therefore decrease out book set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\n",
    "model_knn.fit(user_rating_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = user_rating_matrix.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD = TruncatedSVD(n_components=12, random_state=123)\n",
    "matrix = SVD.fit_transform(X)\n",
    "print(matrix.shape)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(matrix)\n",
    "corr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_titles = user_rating_matrix.index\n",
    "book_list = list(book_titles)\n",
    "matrix_index = book_list.index(\"Harry Potter and the Prisoner of Azkaban (Book 3)\")\n",
    "book_row  = corr[matrix_index]\n",
    "list(book_titles[(book_row<1.0) & (book_row>0.9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice similar books are being recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now we have a model that can reccomend books based on a title, what we need to do is group by demographic, and make a list of books to recommend, these will include all the positively reviewed books or people in that area as well as inputing those reviews into the book recommender to recommend similar books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want recommended books, lets only consider ratings that are above the average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_demographic = demographic_final_data[demographic_final_data['Book-Rating'] >= demographic_final_data['Avg_Rating']]\n",
    "filtered_demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_demographic.groupby('demographic').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_recommendations(demographic_data, corr_matrix, book_titles ,country=\"Na\", state=\"Na\", city=\"Na\", agegroup=\"Na\", ) -> list:\n",
    "    recommended_books = []\n",
    "    book_list = list(book_titles)\n",
    "    target = f\"{country}-{state}-{city}-{agegroup}\"\n",
    "\n",
    "    target_reviews = demographic_data[demographic_data['demographic']==target]\n",
    "    for index, row in target_reviews.iterrows():\n",
    "        recommended_books.append(row['Book-Title'])\n",
    "        if row['Book-Title'] in book_list:\n",
    "            matrix_index = book_list.index(row['Book-Title'])\n",
    "            book_row = corr_matrix[matrix_index]\n",
    "            related_books = list(book_titles[(book_row<1.0) & (book_row>0.9)])\n",
    "            for book in related_books:\n",
    "                recommended_books.extend(related_books)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return recommended_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_books = book_recommendations(filtered_demographic,corr,user_rating_matrix.index,country = 'bangladesh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(recommended_books))\n",
    "recommended_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Testing via LOOCV:\n",
    "\n",
    "to test the precision of our model, we will employ Leave one out cross validation, that is we will iterate through out users, with more than 20 ratings, choose one target and see based off the other books if we are able to predict that given book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_recommendation(merged_data, corr_matrix, book_titles ,user_ID):\n",
    "\n",
    "    book_list = list(book_titles)\n",
    "    user_reviews = merged_data[merged_data['User-ID']==user_ID]\n",
    "    user_books = []\n",
    "\n",
    "    for index, row in user_reviews.iterrows():\n",
    "        user_books.append(row['Book-Title']) \n",
    "    target_book = user_books[0] # we just take the first book without loss of generality\n",
    "    test_books = user_books[1:]\n",
    "\n",
    "    for book in test_books:\n",
    "        if book in book_list:\n",
    "            matrix_index = book_list.index(book)\n",
    "            book_row = corr_matrix[matrix_index]\n",
    "            related_books = list(book_titles[(book_row<1.0) & (book_row>0.7)])\n",
    "            if target_book in related_books:\n",
    "                return 1\n",
    "\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_count = Final_Dataset.groupby('User-ID').size()\n",
    "users_with_more_than_50_ratings = user_ratings_count[user_ratings_count > 50].index\n",
    "test_targets = Final_Dataset[Final_Dataset['User-ID'].isin(users_with_more_than_50_ratings)]\n",
    "test_targets.sort_values(by='User-ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test_targets['User-ID'].tolist()\n",
    "\n",
    "user_id_set = set(test_ids)\n",
    "user_id_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recommendation(Final_Dataset, corr, user_rating_matrix.index, 274301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for id in user_id_set:\n",
    "    match = test_recommendation(Final_Dataset, corr, user_rating_matrix.index, id)\n",
    "    if match == 0:  \n",
    "        results.append(0)  \n",
    "    else:\n",
    "        results.append(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_array = np.array(results)\n",
    "results_array.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_count_index = []\n",
    "accuracy = []\n",
    "for value in range(10, 301, 10):\n",
    "    review_count_index.append(value)\n",
    "\n",
    "\n",
    "for i in range(len(review_count_index)):\n",
    "    user_ratings_count = Final_Dataset.groupby('User-ID').size()\n",
    "    users_with_more_than_50_ratings = user_ratings_count[user_ratings_count > review_count_index[i]].index\n",
    "    test_targets = Final_Dataset[Final_Dataset['User-ID'].isin(users_with_more_than_50_ratings)]\n",
    "    test_ids = test_targets['User-ID'].tolist()\n",
    "    user_id_set = set(test_ids)\n",
    "    results = []\n",
    "    for id in user_id_set:\n",
    "        match = test_recommendation(Final_Dataset, corr, user_rating_matrix.index, id)\n",
    "        if match == 0:  \n",
    "            results.append(0)  \n",
    "        else:\n",
    "            results.append(1)\n",
    "    results_array = np.array(results)\n",
    "    mean = results_array.mean()\n",
    "    accuracy.append(mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(review_count_index, accuracy, marker='o', linestyle='-')\n",
    "plt.title('Accuracy vs. Review Count')\n",
    "plt.xlabel('Review Count Index')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
