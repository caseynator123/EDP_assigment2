{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing BX-Books.csv\n",
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/caseyhaseloff/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/caseyhaseloff/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/caseyhaseloff/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/caseyhaseloff/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Initialize the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# vectorizing the book info column using TFidf Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BX-NewBooksUsers.csv',\n",
       " 'BX-Books.csv',\n",
       " 'BX-Ratings.csv',\n",
       " 'BX-Users.csv',\n",
       " 'BX-NewBooksRatings.csv',\n",
       " 'BX-NewBooks.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(os.path.normpath(os.getcwd() + os.sep + os.pardir) + \"/data/\")\n",
    "\n",
    "os.listdir(path + 'raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match_comparison(col):\n",
    "    \n",
    "    unique = books[f\"Book-{col}\"].unique()\n",
    "    print(len(unique))\n",
    "    \n",
    "    score_sort = [(x,) + i\n",
    "             for x in unique\n",
    "             for i in process.extract(x, unique, scorer=fuzz.ratio)] \n",
    "    \n",
    "    similarity_sort = pd.DataFrame(score_sort, columns=[f'{col}_sort','match_sort','score_sort'])\n",
    "    similarity_sort[f'sorted_{col}_sort'] = np.minimum(similarity_sort[f'{col}_sort'], similarity_sort['match_sort'])\n",
    "\n",
    "    high_score_sort = \\\n",
    "    similarity_sort[(similarity_sort['score_sort'] >= 80) & \\\n",
    "                    (similarity_sort[f'{col}_sort'] !=  similarity_sort['match_sort']) & \\\n",
    "                    (similarity_sort[f'sorted_{col}_sort'] != similarity_sort['match_sort'])] \n",
    "    high_score_sort = high_score_sort.drop(f'sorted_{col}_sort',axis=1).copy()\n",
    "\n",
    "    high_score_sort = high_score_sort.sort_values(\n",
    "                        ['score_sort'], ascending=False).reset_index()\n",
    "\n",
    "    return high_score_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(path + 'raw/' + 'BX-Books.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Book Author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex pattern to remove non-alphanumeric characters except '&'\n",
    "pattern = r'[^a-zA-Z0-9& ]'\n",
    "\n",
    "# Function to clean text based on regex pattern\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "books[\"Book-Author-Processed\"] = books[\"Book-Author\"].str.lower()\n",
    "books[\"Book-Author-Processed\"] = books[\"Book-Author-Processed\"].str.replace(\"'\", \"\")\n",
    "books[\"Book-Author-Processed\"] = books[\"Book-Author-Processed\"].apply(clean_text).str.replace('  ', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5960\n",
      "CPU times: user 52.8 s, sys: 1.28 s, total: 54.1 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "author_fuzzy_match = fuzzy_match_comparison(\"Author-Processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Author-Processed_sort</th>\n",
       "      <th>match_sort</th>\n",
       "      <th>score_sort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10836</td>\n",
       "      <td>friedrich duerenmatt</td>\n",
       "      <td>friedrich duerrenmatt</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20936</td>\n",
       "      <td>jean christophe grang</td>\n",
       "      <td>jean christophe grange</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28551</td>\n",
       "      <td>bathroom reader institute</td>\n",
       "      <td>bathroom readers institute</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29551</td>\n",
       "      <td>gabriel garacia marquez</td>\n",
       "      <td>gabriel garcia marquez</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18006</td>\n",
       "      <td>judith michael</td>\n",
       "      <td>judith michaels</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>21766</td>\n",
       "      <td>garth ennis</td>\n",
       "      <td>garth nix</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>21666</td>\n",
       "      <td>joe connelly</td>\n",
       "      <td>john connolly</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>21506</td>\n",
       "      <td>d j conway</td>\n",
       "      <td>deanna j conway</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>8691</td>\n",
       "      <td>harry harrison</td>\n",
       "      <td>kathryn harrison</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>7721</td>\n",
       "      <td>bill phillips</td>\n",
       "      <td>bob phillips</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1020 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      Author-Processed_sort                  match_sort  score_sort\n",
       "0     10836       friedrich duerenmatt       friedrich duerrenmatt          98\n",
       "1     20936      jean christophe grang      jean christophe grange          98\n",
       "2     28551  bathroom reader institute  bathroom readers institute          98\n",
       "3     29551    gabriel garacia marquez      gabriel garcia marquez          98\n",
       "4     18006             judith michael             judith michaels          97\n",
       "...     ...                        ...                         ...         ...\n",
       "1015  21766                garth ennis                   garth nix          80\n",
       "1016  21666               joe connelly               john connolly          80\n",
       "1017  21506                 d j conway             deanna j conway          80\n",
       "1018   8691             harry harrison            kathryn harrison          80\n",
       "1019   7721              bill phillips                bob phillips          80\n",
       "\n",
       "[1020 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_fuzzy_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'friedrich duerenmatt': 'friedrich duerrenmatt',\n",
       " 'jean christophe grang': 'jean christophe grange',\n",
       " 'bathroom reader institute': 'bathroom readers institute',\n",
       " 'gabriel garacia marquez': 'gabriel garcia marquez',\n",
       " 'judith michael': 'judith michaels',\n",
       " 'fyodor dostoevsky': 'fyodor dostoyevsky',\n",
       " 'zalata filipovic': 'zlata filipovic',\n",
       " 'frederic beigbeder': 'frederick beigbeder',\n",
       " 'philip k howard': 'phillip k howard',\n",
       " 'beverley donofrio': 'beverly donofrio',\n",
       " 'william shakespeare': 'william shakspeare',\n",
       " 'f scott fitzgerald': 'f scott fritzgerald',\n",
       " 'vonda n mcintryre': 'vonda n mcintyre',\n",
       " 'ursula k le guin': 'ursula k leguin',\n",
       " 'laura ingall wilder': 'laura ingalls wilder',\n",
       " 'carol matthews': 'carole matthews',\n",
       " 'mariann fredriksson': 'marianne fredriksson',\n",
       " 'berhard schlink': 'bernhard schlink',\n",
       " 'elizabet coatsworth': 'elizabeth coatsworth',\n",
       " 'diana wynne jones': 'dianna wynne jones',\n",
       " 'barbara de angelis': 'barbara deangelis',\n",
       " 'daniel de foe': 'daniel defoe',\n",
       " 'john le carre': 'john lecarre',\n",
       " 'frances hodgson burnett': 'francis hodgson burnett',\n",
       " 'jon schieszka': 'jon scieszka',\n",
       " 'paul coelho': 'paulo coelho',\n",
       " 'susana tamaro': 'susanna tamaro',\n",
       " 'benjamin v stuckrad barre': 'benjamin von stuckrad barre',\n",
       " 'kenzaburo o': 'kenzaburo oe',\n",
       " 'stephen frey': 'stephen w frey',\n",
       " 'thomas moore': 'thomas more',\n",
       " 'george perec': 'georges perec',\n",
       " 'elliot hester': 'elliott hester',\n",
       " 'steve martin': 'steve martini',\n",
       " 'rob mac gregor': 'rob macgregor',\n",
       " 'gaby hauptman': 'gaby hauptmann',\n",
       " 'susan power': 'susan powter',\n",
       " 'milan kundera': 'milans kundera',\n",
       " 'john le carr': 'john lecarre',\n",
       " 'peter heg': 'peter hoeg',\n",
       " 'bathroom readers hysterical society': 'the bathroom readers hysterical society',\n",
       " 'katherine a applegate': 'katherine applegate',\n",
       " 'a s  byatt': 'a s byatt',\n",
       " 'marion zimm bradley': 'marion zimmer bradley',\n",
       " 'iain banks': 'ian banks',\n",
       " 'bill hayes': 'billy hayes',\n",
       " 'max barry': 'maxx barry',\n",
       " 'elsa holmelund minarik': 'else holmelund minarik',\n",
       " 'kathleen e woodiwiss': 'kathleen woodiwiss',\n",
       " 't coraghessan boyle': 'tom coraghessan boyle',\n",
       " 'friedric durrenmatt': 'friedrich duerenmatt',\n",
       " 'laura c schlessinger': 'laura schlessinger',\n",
       " 'william j kennedy': 'william kennedy',\n",
       " 'barbara e johnson': 'barbara johnson',\n",
       " 'stephen donaldson': 'stephen r donaldson',\n",
       " 'barbara tuchman': 'barbara w tuchman',\n",
       " 'phillip m margolin': 'phillip margolin',\n",
       " 'martin greenberg': 'martin h greenberg',\n",
       " 'michael cunningham': 'micheal cunningham',\n",
       " 'patrick j mcmanus': 'patrick r mcmanus',\n",
       " 'doris m lessing': 'doris may lessing',\n",
       " 'iris rainer dart': 'iris ranier dart',\n",
       " 'patricia cornwell': 'patricia d cornwell',\n",
       " 'james finn garner': 'james pinn garner',\n",
       " 'robert a heinlein': 'robert heinlein',\n",
       " 'richard a wright': 'richard wright',\n",
       " 'shirley maclaine': 'shirley mcclaine',\n",
       " 'patrick f mcmanus': 'patrick j mcmanus',\n",
       " 'adriana trigiana': 'adriana trigiani',\n",
       " 'william coughlin': 'william j coughlin',\n",
       " 'patrick mcmanus': 'patrick r mcmanus',\n",
       " 'perri oshaughnessy': 'perry oshaughnessy',\n",
       " 'marilyn robinson': 'marilynne robinson',\n",
       " 'stephen ambrose': 'stephen e ambrose',\n",
       " 'robert mccammon': 'robert r mccammon',\n",
       " 'edward rutherford': 'edward rutherfurd',\n",
       " 'stephen hawking': 'stephen w hawking',\n",
       " 'william burroughs': 'william s burroughs',\n",
       " 'john d fitzgerald': 'john fitzgerald',\n",
       " 'george p pelecanos': 'george pelecanos',\n",
       " 'antoine de saint exup  ry': 'antoine de saint exupery',\n",
       " 'joseph f girzone': 'joseph girzone',\n",
       " 'robert a wilson': 'robert wilson',\n",
       " 'elizabeth boyle': 'elizabeth doyle',\n",
       " 'richard b wright': 'richard wright',\n",
       " 'raymond e feist': 'raymond feist',\n",
       " 'thich nhat hahn': 'thich nhat hanh',\n",
       " 'gabr garcia marquez': 'gabriel garcia marquez',\n",
       " 'andrew greeley': 'andrew m greeley',\n",
       " 'steven boyett': 'steven r boyett',\n",
       " 'richard a knaak': 'richard knaak',\n",
       " 'leo buscaglia': 'leo f buscaglia',\n",
       " 'james a michener': 'james michener',\n",
       " 'andrew h vachss': 'andrew vachss',\n",
       " 'james l burke': 'james lee burke',\n",
       " 'robert m pirsig': 'robert pirsig',\n",
       " 'harold kushner': 'harold s kushner',\n",
       " 'joel c rosenberg': 'joel rosenberg',\n",
       " 'arturo p  rez reverte': 'arturo perez reverte',\n",
       " 'john lescroart': 'john t lescroart',\n",
       " 'joseph heller': 'joseph l heller',\n",
       " 'robert asprin': 'robert l asprin',\n",
       " 'michael dorris': 'michael morris',\n",
       " 'matthew j reilly': 'matthew reilly',\n",
       " 'jerry b jenkins': 'jerry jenkins',\n",
       " 'doris lessing': 'doris m lessing',\n",
       " 'frank e peretti': 'frank peretti',\n",
       " 'patricia c wrede': 'patricia wrede',\n",
       " 'norman f maclean': 'norman maclean',\n",
       " 'antwone fisher': 'antwone q fisher',\n",
       " 'jeffery deaver': 'jeffrey deaver',\n",
       " 'octavia butler': 'octavia e butler',\n",
       " 'robert b parker': 'robert parker',\n",
       " 'robert munsch': 'robert n munsch',\n",
       " 'jared diamond': 'jared m diamond',\n",
       " 'dr hunter s thompson': 'hunter s thompson',\n",
       " 'judith r hendricks': 'judith ryan hendricks',\n",
       " 'jack b du brul': 'jack du brul',\n",
       " 'barbara park': 'barbara parker',\n",
       " 'jim garrison': 'jim harrison',\n",
       " 'carolyn g hart': 'carolyn hart',\n",
       " 'john douglas': 'john e douglas',\n",
       " 'john gunther': 'john j gunther',\n",
       " 'james huston': 'james w huston',\n",
       " 'betina krahn': 'betina m krahn',\n",
       " 'robert bright': 'robert wright',\n",
       " 'walter j lord': 'walter lord',\n",
       " 'torey hayden': 'torey l hayden',\n",
       " 'paul fussell': 'paul russell',\n",
       " 'john edward': 'john j edward',\n",
       " 'peter beagle': 'peter s beagle',\n",
       " 'stefan zweig': 'stefanie zweig',\n",
       " 'rachel simmons': 'rachel simon',\n",
       " 'oliver sacks': 'oliver w sacks',\n",
       " 'annie proulx': 'e annie proulx',\n",
       " 'j lynne hinton': 'lynne hinton',\n",
       " 'helen myers': 'helen r myers',\n",
       " 'james c dobson': 'james dobson',\n",
       " 'jim harrison': 'kim harrison',\n",
       " 'niccol   machiavelli': 'niccolo machiavelli',\n",
       " 'wilbur a smith': 'wilbur smith',\n",
       " 'anna l waldo': 'anna lee waldo',\n",
       " 'patricia a mckillip': 'patricia mc killip',\n",
       " 'john martel': 'john s martel',\n",
       " 'poppy brite': 'poppy z brite',\n",
       " 'f paul wilson': 'paul wilson',\n",
       " 'james burke': 'james l burke',\n",
       " 'andrea brown': 'sandra brown',\n",
       " 'jean p sasson': 'jean sasson',\n",
       " 'peter abrahams': 'peter abrams',\n",
       " 'dean koontz': 'dean r koontz',\n",
       " 'paolo coelho': 'paulo coelho'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff = 92\n",
    "author_fuzzy_dict = author_fuzzy_match.loc[author_fuzzy_match[\"score_sort\"]  >= cutoff]\n",
    "author_fuzzy_dict = author_fuzzy_dict.set_index(\"Author-Processed_sort\")\n",
    "author_fuzzy_dict = author_fuzzy_dict[\"match_sort\"]\n",
    "author_fuzzy_dict.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5813\n"
     ]
    }
   ],
   "source": [
    "books[\"Book-Author-Processed\"] = books[\"Book-Author-Processed\"].replace(author_fuzzy_dict)\n",
    "\n",
    "unique = books[\"Book-Author-Processed\"].unique()\n",
    "print(len(unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Book Publisher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[\"Book-Publisher-Processed\"] = books[\"Book-Publisher\"].str.lower()\n",
    "books[\"Book-Publisher-Processed\"] = books[\"Book-Publisher-Processed\"].str.replace(\"'\", \"\")\n",
    "books[\"Book-Publisher-Processed\"] = books[\"Book-Publisher-Processed\"].apply(clean_text).str.replace('  ', ' ').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1319\n",
      "CPU times: user 2.76 s, sys: 37.2 ms, total: 2.8 s\n",
      "Wall time: 2.87 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Publisher-Processed_sort</th>\n",
       "      <th>match_sort</th>\n",
       "      <th>score_sort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3091</td>\n",
       "      <td>ullstein buchverlage gmbh &amp; co kg  ullstein tas</td>\n",
       "      <td>ullstein buchverlage gmbh &amp; co kg  ullstein tasc</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1151</td>\n",
       "      <td>droemersche verlagsanstalt th knaur nachf  gmb...</td>\n",
       "      <td>droemersche verlagsanstalt th knaur nachf gmbh...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>266</td>\n",
       "      <td>harpercollins publisher</td>\n",
       "      <td>harpercollins publishers</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4906</td>\n",
       "      <td>harper collins canada</td>\n",
       "      <td>harpercollins canada</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3796</td>\n",
       "      <td>harper collins publishers</td>\n",
       "      <td>harpercollins publishers</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>5062</td>\n",
       "      <td>haynes publications</td>\n",
       "      <td>hysteria publications</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>429</td>\n",
       "      <td>knopf books for young readers</td>\n",
       "      <td>random house books for young readers</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>5023</td>\n",
       "      <td>alyson publications</td>\n",
       "      <td>arc publications</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1298</td>\n",
       "      <td>pan books ltd</td>\n",
       "      <td>penguin books ltd</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>4451</td>\n",
       "      <td>algonquin books</td>\n",
       "      <td>harlequin books</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  ... score_sort\n",
       "0     3091  ...         99\n",
       "1     1151  ...         99\n",
       "2      266  ...         98\n",
       "3     4906  ...         98\n",
       "4     3796  ...         98\n",
       "..     ...  ...        ...\n",
       "427   5062  ...         80\n",
       "428    429  ...         80\n",
       "429   5023  ...         80\n",
       "430   1298  ...         80\n",
       "431   4451  ...         80\n",
       "\n",
       "[432 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "publisher_fuzzy_match = fuzzy_match_comparison(\"Publisher-Processed\")\n",
    "publisher_fuzzy_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ullstein buchverlage gmbh & co kg  ullstein tas': 'ullstein buchverlage gmbh & co kg  ullstein tasc',\n",
       " 'droemersche verlagsanstalt th knaur nachf  gmbh & co': 'droemersche verlagsanstalt th knaur nachf gmbh & co',\n",
       " 'harpercollins publisher': 'harpercollins publishers',\n",
       " 'harper collins canada': 'harpercollins canada',\n",
       " 'harper collins publishers': 'harpercollins publisher',\n",
       " 'harper san francisco': 'harpersanfrancisco',\n",
       " 'harper sanfrancisco': 'harpersanfrancisco',\n",
       " 'berkeley books': 'berkley books',\n",
       " 'schoenhof foreign books inc': 'schoenhofsforeign books inc',\n",
       " 'signet book': 'signet books',\n",
       " 'tyndale house publishers': 'tyndale house putlishers',\n",
       " 'da capo press': 'dacapo press',\n",
       " 'laure leaf': 'laurel leaf',\n",
       " 'harper mass market paperbacks': 'harper mass market paperbacks mm',\n",
       " 'regan books': 'reganbooks',\n",
       " 'random house childrens pub': 'random house childrens pub mm',\n",
       " 'penguin puffin mass market': 'penguin puffin mass market mm',\n",
       " 'little brown & company': 'little brown and company',\n",
       " 'farrar straus & giroux': 'farrar straus and giroux',\n",
       " 'scholastic paperbacks': 'scholastic paperbacks mm',\n",
       " 'new amer library classics': 'new amer library classics mm',\n",
       " 'scholastic paperbacks mm': 'scholastic paperbacks t',\n",
       " 'crown publishing group': 'orion publishing group',\n",
       " 'harper collins  uk': 'harpercollins uk',\n",
       " 'deutscher taschenbuch verlag': 'deutscher taschenbuch verlag dtv',\n",
       " 'editions 10 18': 'editions 10 19',\n",
       " 'simon & schuster': 'simon schuster',\n",
       " 'lectorum pubns': 'lectorum pubns j',\n",
       " 'alfred a knopf': 'alfred e knopf',\n",
       " 'random house': 'random house p',\n",
       " 'barnes & noble': 'barnes noble',\n",
       " 'penguin usa': 'penguin usa p',\n",
       " 'farrar straus and giroux': 'farrar straus giroux',\n",
       " 'random house childrens': 'random house childrens pub',\n",
       " 'disinformation company': 'the disinformation company',\n",
       " 'faber & faber': 'faber faber',\n",
       " 'barnes & nobles books': 'barnes noble books',\n",
       " 'berkley pub group': 'berkley pub group mm',\n",
       " 'penguin u s a': 'penguin usa',\n",
       " 'three rivers press': 'three rivers press ca',\n",
       " 'harper & row': 'harper row',\n",
       " 'henry holt & company': 'henry holt & company inc',\n",
       " 'lectorum publications': 'lectorum publications inc',\n",
       " 'little brown and company': 'little brown company',\n",
       " 'fischer taschenbuch verlag': 'fischer taschenbuch verlag gmbh',\n",
       " 'w w norton & company': 'w w norton & company ltd',\n",
       " 'baker book house': 'baker book house co',\n",
       " 'editorial seix barral': 'editorial seix barral s a',\n",
       " 'houghton mifflin': 'houghton mifflin co',\n",
       " 'diogenes verlag': 'diogenes verlag ag',\n",
       " 'new amer library': 'new amer library mm',\n",
       " 'mcclelland & stewart': 'mcclelland & stewart ltd',\n",
       " 'health communications': 'health communications inc'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff = 91\n",
    "publisher_fuzzy_dict = publisher_fuzzy_match.loc[publisher_fuzzy_match[\"score_sort\"]  >= cutoff]\n",
    "publisher_fuzzy_dict = publisher_fuzzy_dict.set_index(\"Publisher-Processed_sort\")\n",
    "publisher_fuzzy_dict = publisher_fuzzy_dict[\"match_sort\"]\n",
    "publisher_fuzzy_dict.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272\n"
     ]
    }
   ],
   "source": [
    "books[\"Book-Publisher-Processed\"] = books[\"Book-Publisher-Processed\"].replace(publisher_fuzzy_dict)\n",
    "\n",
    "unique = books[\"Book-Publisher-Processed\"].unique()\n",
    "print(len(unique))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "books[\"Book-Publisher\"] = books[\"Book-Publisher-Processed\"]\n",
    "books[\"Book-Author\"] = books[\"Book-Author-Processed\"]\n",
    "books = books.drop([\"Book-Publisher-Processed\", \"Book-Author-Processed\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Book Title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # going to keep stop words in since there are titles with only stop words and everything in title is important\n",
    "    # Remove stop words\n",
    "    # tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Return the processed text as a string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df, column_name):\n",
    "    df[f\"{column_name}-Processed\"] = df[column_name].apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = preprocess_dataframe(books, \"Book-Title\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "books[\"Book-Title\"] = books[\"Book-Title-Processed\"]\n",
    "books = books.drop([\"Book-Title-Processed\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF for Book-Title:\n",
    "Vectorise the book table so we can we can compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['Book-Info'] = books['Book-Title-Processed'] + ' by ' + books['Book-Author-Processed'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer = \"word\", ngram_range=(1,2), min_df=0, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tf.fit(books['Book-Info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = tfidf_matrix.transform(books['Book-Info']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['Book-Vector'] = list(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_verify(year):\n",
    "    min_year = 1000\n",
    "    max_year = 2024\n",
    "    return (year >= min_year) & (year <= max_year)\n",
    "\n",
    "raw_year = books[\"Year-Of-Publication\"]\n",
    "real_year = raw_year.loc[year_verify(raw_year)]\n",
    "general_median_year = real_year.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of years imputated by grouping Title-Processed: 81\n",
      "Number of years imputated by grouping Author-Processed: 153\n",
      "Number of years imputated by grouping Publisher-Processed: 64\n",
      "Number of years imputated by grouping All Data: 19\n"
     ]
    }
   ],
   "source": [
    "def median_group(group, index):\n",
    "    book_group = books[[group, \"Year-Of-Publication\"]]\n",
    "    book_group = book_group.loc[book_group[group] == book_group[group][index]]\n",
    "    median_year = book_group[\"Year-Of-Publication\"]\n",
    "    median_year = median_year.loc[year_verify(median_year)]\n",
    "    median_year = median_year.median()\n",
    "    return median_year\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "imputation_method = dd(int)\n",
    "\n",
    "def imputate_year(index):\n",
    "    year = books[\"Year-Of-Publication\"][index]\n",
    "    if (year_verify(year)):\n",
    "        return year\n",
    "\n",
    "    groups = [\"Book-Title-Processed\", \"Book-Author-Processed\", \"Book-Publisher-Processed\"]\n",
    "    for group in groups:\n",
    "        year = median_group(group, index)\n",
    "        if (year_verify(year)):\n",
    "            imputation_method[group] += 1\n",
    "            return year\n",
    "        \n",
    "    imputation_method[\"Book-All Data\"] += 1\n",
    "    return general_median_year \n",
    "\n",
    "books[\"Year-Of-Publication-Processed\"] = pd.Series(books.index).apply(imputate_year)\n",
    "for key in [\"Title-Processed\", \"Author-Processed\", \"Publisher-Processed\", \"All Data\"]: \n",
    "    print(f'Number of years imputated by grouping {key}: {imputation_method[\"Book-\"+key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[\"Year-Of-Publication\"] = books[\"Year-Of-Publication-Processed\"]\n",
    "books = books.drop([\"Year-Of-Publication-Processed\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize Years into Decades:\n",
    "Reasoning for this is that types of users could like books from a certain decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_list = [1919 + 10 * x for x in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['Year-Of-Publication-Group'] = pd.cut(x=books['Year-Of-Publication'], bins=decade_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "encoded_publish_year = encoder.fit_transform(books[['Year-Of-Publication-Group']])\n",
    "books[['Year-Of-Publication-Group-Encoded']] = encoded_publish_year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Book-Publisher</th>\n",
       "      <th>Book-Author-Processed</th>\n",
       "      <th>Book-Publisher-Processed</th>\n",
       "      <th>Book-Title-Processed</th>\n",
       "      <th>Book-Info</th>\n",
       "      <th>Book-Vector</th>\n",
       "      <th>Year-Of-Publication-Group</th>\n",
       "      <th>Year-Of-Publication-Group-Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>richard bruce wright</td>\n",
       "      <td>harperflamingo canada</td>\n",
       "      <td>clara callan</td>\n",
       "      <td>clara callan by richard bruce wright</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1999, 2009]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN  ... Year-Of-Publication-Group-Encoded\n",
       "0  0002005018  ...                                 8\n",
       "\n",
       "[1 rows x 12 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_csv(path + 'cleaned/' + \"BX-Books.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
