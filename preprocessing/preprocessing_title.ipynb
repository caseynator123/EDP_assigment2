{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf2793e-9c17-4f66-a620-47f975fd10c2",
   "metadata": {},
   "source": [
    "# Book-Title preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25d164-258f-4163-8128-5c283f6d7861",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84e49b03-f497-4611-9423-5beb27b4b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\andyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Initialize the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# vectorizing the book info column using TFidf Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c86f0cd-ce87-4075-bcdc-7f7d27e48b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.normpath(os.getcwd() + os.sep + os.pardir) + \"/data/raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82dda1a8-a7fc-4872-98eb-b702a0be007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.path.join(os.path.normpath(os.getcwd() + os.sep + os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6313679-606d-4570-b43a-7752daf3f703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BX-Books.csv',\n",
       " 'BX-NewBooks.csv',\n",
       " 'BX-NewBooksRatings.csv',\n",
       " 'BX-NewBooksUsers.csv',\n",
       " 'BX-Ratings.csv',\n",
       " 'BX-Users.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b4870a8-ef21-4d90-8ddb-8243a8c818ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(path + 'BX-Books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80001f0b-fc4e-4c8d-b3b9-87b0f6b6f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Return the processed text as a string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def preprocess_dataframe(df, column_name):\n",
    "    df[f\"{column_name}\"] = df[column_name].apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe91c44b-b879-435c-97df-059668ba20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = preprocess_dataframe(books, \"Book-Title\")\n",
    "books = preprocess_dataframe(books, \"Book-Publisher\")\n",
    "books = preprocess_dataframe(books, \"Book-Author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d9c56b7-f834-4940-b435-1f66513006c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Book-Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>clara callan</td>\n",
       "      <td>richard bruce wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>harperflamingo canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>flu story great influenza pandemic 1918 search...</td>\n",
       "      <td>gina bari kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>farrar straus giroux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0399135782</td>\n",
       "      <td>kitchen god wife</td>\n",
       "      <td>amy tan</td>\n",
       "      <td>1991</td>\n",
       "      <td>putnam pub group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0440234743</td>\n",
       "      <td>testament</td>\n",
       "      <td>john grisham</td>\n",
       "      <td>1999</td>\n",
       "      <td>dell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0452264464</td>\n",
       "      <td>beloved plume contemporary fiction</td>\n",
       "      <td>toni morrison</td>\n",
       "      <td>1994</td>\n",
       "      <td>plume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>0375411615</td>\n",
       "      <td>love etc</td>\n",
       "      <td>julian barnes</td>\n",
       "      <td>2001</td>\n",
       "      <td>alfred knopf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>0836227751</td>\n",
       "      <td>wit whimsy mary engelbreit</td>\n",
       "      <td>mary engelbreit</td>\n",
       "      <td>1997</td>\n",
       "      <td>andrew mcmeel publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18182</th>\n",
       "      <td>8433966634</td>\n",
       "      <td>los detective salvajes</td>\n",
       "      <td>roberto bolano</td>\n",
       "      <td>2003</td>\n",
       "      <td>anagrama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18183</th>\n",
       "      <td>0330353349</td>\n",
       "      <td>ice house tv tie edition</td>\n",
       "      <td>minette walter</td>\n",
       "      <td>1997</td>\n",
       "      <td>mcclelland stewart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18184</th>\n",
       "      <td>0394757645</td>\n",
       "      <td>trouble business vintage crime black lizard</td>\n",
       "      <td>raymond chandler</td>\n",
       "      <td>1992</td>\n",
       "      <td>vintage book usa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18185 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ISBN                                         Book-Title  \\\n",
       "0      0002005018                                       clara callan   \n",
       "1      0374157065  flu story great influenza pandemic 1918 search...   \n",
       "2      0399135782                                   kitchen god wife   \n",
       "3      0440234743                                          testament   \n",
       "4      0452264464                 beloved plume contemporary fiction   \n",
       "...           ...                                                ...   \n",
       "18180  0375411615                                           love etc   \n",
       "18181  0836227751                         wit whimsy mary engelbreit   \n",
       "18182  8433966634                             los detective salvajes   \n",
       "18183  0330353349                           ice house tv tie edition   \n",
       "18184  0394757645        trouble business vintage crime black lizard   \n",
       "\n",
       "                Book-Author  Year-Of-Publication            Book-Publisher  \n",
       "0      richard bruce wright                 2001     harperflamingo canada  \n",
       "1          gina bari kolata                 1999      farrar straus giroux  \n",
       "2                   amy tan                 1991          putnam pub group  \n",
       "3              john grisham                 1999                      dell  \n",
       "4             toni morrison                 1994                     plume  \n",
       "...                     ...                  ...                       ...  \n",
       "18180         julian barnes                 2001              alfred knopf  \n",
       "18181       mary engelbreit                 1997  andrew mcmeel publishing  \n",
       "18182        roberto bolano                 2003                  anagrama  \n",
       "18183        minette walter                 1997        mcclelland stewart  \n",
       "18184      raymond chandler                 1992          vintage book usa  \n",
       "\n",
       "[18185 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f277258-e6d3-4b8c-8cde-ba3aa4163cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_csv(\"BX-Cleaned-Books.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
